{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LSTM on Perez data, using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miketp333/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS BEGIN ###############################################\n",
    "MAX_ARTICLE_LENGTH = 1000\n",
    "EMBEDDING_VECTOR_LENGTH = 50\n",
    "EMBEDDING_VOCAB_SIZE = 400000\n",
    "LSTM_MEMORY_SIZE = 100\n",
    "NN_OPTIMIZER = 'adam'\n",
    "NN_LOSS_FUNCTION = 'binary_crossentropy'\n",
    "NN_EPOCHS = 3\n",
    "USE_GLOVE_EMBEDDINGS = False\n",
    "NN_BATCH_SIZE = 128\n",
    "# HYPERPARAMETERS END #################################################\n",
    "\n",
    "# Other config parameters\n",
    "RANDOM_SEED = 42\n",
    "GLOVE_FILEPATH = 'models/embeddings/glove.6B.%dd.txt' % EMBEDDING_VECTOR_LENGTH\n",
    "FR_DATASET_PATH = \"data/fakerealnews_GeorgeMcIntire/fake_or_real_news.csv\"\n",
    "PEREZ_DATASET_PATH = \"data/fakeNewsDatasets_Perez-Rosas2018\"\n",
    "ID_UNKNOWN = 399999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "\n",
    "def load_glove_model_v2(dim):\n",
    "    \"\"\"Load a Glove model into a gensim model, converting it\n",
    "    into word2vec if necessary.\n",
    "    Adapted from: https://stackoverflow.com/a/47465278\n",
    "    \"\"\"\n",
    "    print(\"Loading Glove embedding\")\n",
    "    glove_data_file = GLOVE_FILEPATH\n",
    "    word2vec_output_file = '%s.w2v' % glove_data_file\n",
    "\n",
    "    if not Path(word2vec_output_file).exists():\n",
    "        glove2word2vec(glove_input_file=glove_data_file, word2vec_output_file=word2vec_output_file)\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    print(\"Loaded Glove embedding\")\n",
    "\n",
    "    embedding_matrix = np.zeros((len(model.vocab), dim))\n",
    "    for i in range(len(model.vocab)):\n",
    "        embedding_vector = model[model.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return model, embedding_matrix\n",
    "\n",
    "\n",
    "def article_to_word_id_list(article, model):\n",
    "    word_index_list = []\n",
    "    word_list = article.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word in model.vocab:\n",
    "            word_index_list.append(model.vocab[word].index)\n",
    "        else:\n",
    "            # Unknown\n",
    "            word_index_list.append(ID_UNKNOWN)\n",
    "    return word_index_list\n",
    "\n",
    "def read_mcintire_dataset():\n",
    "    print(\"Reading dataset\")\n",
    "    fr = pd.read_csv(FR_DATASET_PATH)\n",
    "    fr = fr.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    fr['title_and_text'] = fr['title'] + ' ' + fr['text']\n",
    "    model, embedding_matrix = load_glove_model_v2(EMBEDDING_VECTOR_LENGTH)\n",
    "    fr['title_and_text_cleaned'] = fr['title_and_text'].apply(lambda a: cleanArticle(a))\n",
    "    fr['news_embed_idx'] = fr['title_and_text_cleaned'].apply(lambda a: article_to_word_id_list(a, model))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(fr['news_embed_idx'], np.where(fr['label'] == 'FAKE', 1, 0),\n",
    "                         test_size=.2, random_state=RANDOM_SEED)\n",
    "\n",
    "    print(\"Finished reading dataset\")\n",
    "    return X_train, X_test, y_train, y_test, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_perez_dataset(dataset_name):\n",
    "    \n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "    \n",
    "    print(\"Reading dataset\")\n",
    "    result_data_list = []\n",
    "    data_dir = PEREZ_DATASET_PATH\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data_list.append(result_data)\n",
    "                \n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    \n",
    "    model, embedding_matrix = load_glove_model_v2(EMBEDDING_VECTOR_LENGTH)\n",
    "    df['news_all_clean'] = df['news_all'].apply(lambda a: cleanArticle(a))\n",
    "    df['news_embed_idx'] = df['news_all_clean'].apply(lambda a: article_to_word_id_list(a, model))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['news_embed_idx'], df['is_fake'], \n",
    "                                                        test_size=.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Finished reading dataset\")\n",
    "    return X_train, X_test, y_train, y_test, embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Loading Glove embedding\n",
      "Loaded Glove embedding\n",
      "Finished reading dataset\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "X_train, X_test, y_train, y_test, embedding_matrix = read_perez_dataset('fakeNewsDataset')\n",
    "# X_train, X_test, y_train, y_test, embedding_matrix = read_mcintire_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          20000000  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,060,501\n",
      "Trainable params: 20,060,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Add padding if needed\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_ARTICLE_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_ARTICLE_LENGTH)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "if USE_GLOVE_EMBEDDINGS:\n",
    "    model.add(Embedding(EMBEDDING_VOCAB_SIZE, EMBEDDING_VECTOR_LENGTH, weights=[embedding_matrix],\n",
    "                        input_length=MAX_ARTICLE_LENGTH, trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(EMBEDDING_VOCAB_SIZE, EMBEDDING_VECTOR_LENGTH, input_length=MAX_ARTICLE_LENGTH))\n",
    "\n",
    "# Question: How to decide what initializers to use?\n",
    "# Added multiple layers\n",
    "# model.add(LSTM(LSTM_MEMORY_SIZE, return_sequences=True, input_shape=(MAX_ARTICLE_LENGTH, EMBEDDING_VECTOR_LENGTH)))\n",
    "# model.add(LSTM(LSTM_MEMORY_SIZE, return_sequences=True))\n",
    "model.add(LSTM(LSTM_MEMORY_SIZE))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=NN_LOSS_FUNCTION, optimizer=NN_OPTIMIZER, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 384 samples, validate on 96 samples\n",
      "Epoch 1/3\n",
      "384/384 [==============================] - 10s 27ms/step - loss: 0.6932 - acc: 0.4844 - val_loss: 0.6925 - val_acc: 0.5521\n",
      "Epoch 2/3\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.6908 - acc: 0.6562 - val_loss: 0.6922 - val_acc: 0.6146\n",
      "Epoch 3/3\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.6874 - acc: 0.8073 - val_loss: 0.6919 - val_acc: 0.5312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcacd037c50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=NN_EPOCHS, batch_size=NN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 1s 7ms/step\n",
      "Accuracy: 53.12%\n"
     ]
    }
   ],
   "source": [
    "# Predict model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 34],\n",
       "       [11, 36]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix of results (ensure it doesn't predict the same class for all records)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
