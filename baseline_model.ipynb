{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook implements model that makes predictions based on word counts, using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries.\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import datetime\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# NLTK APIs\n",
    "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "\n",
    "# Function to load news articles\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = datapath\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load fake news data\n",
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "newsVectors, newsVectors_test, classVector, classVector_test = \\\n",
    "    train_test_split(fakenews_df['news_all'],\n",
    "                     fakenews_df['is_fake'], \n",
    "                     test_size = .2,\n",
    "                     random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stemming and lemmatization\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=get_wordnet_pos(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', 'numseq', text)\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    filtered_text = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_text.append(w) \n",
    "            \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=preprocess_text)\n",
    "train_data = vectorizer.fit_transform(newsVectors)\n",
    "test_data = vectorizer.transform(newsVectors_test)\n",
    "\n",
    "print(\"Size of the vocabulary:\", train_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial modeling\n",
    "alphas = {'alpha': [0.01, 0.05, 0.1, 0.5, 0.8, 1, 1.5, 5, 10]}\n",
    "multi_gs = GridSearchCV(estimator=MultinomialNB(), param_grid=alphas)\n",
    "multi_gs.fit(train_data, classVector)\n",
    "\n",
    "multi_clf_best = MultinomialNB(alpha=multi_gs.best_params_['alpha'])\n",
    "multi_clf_best.fit(train_data, classVector)\n",
    "multi_clf_best_predicted = multi_clf_best.predict(test_data)\n",
    "fpr, tpr, _ = metrics.roc_curve(classVector_test, multi_clf_best_predicted)\n",
    "\n",
    "print('----- Multinomial Naive Bayes -----')\n",
    "print (multi_gs.best_params_)\n",
    "print(\"Multinomial accuracy: {:2.5f}%\".format(accuracy_score(multi_clf_best_predicted, classVector_test) * 100))\n",
    "print(\"Multinomial accuracy (AUC): {:2.5f}%\".format(metrics.auc(fpr,tpr) * 100))\n",
    "\n",
    "print('Number of predictions:', len(multi_clf_best_predicted))\n",
    "print('Number predicted as Fake News:',sum(multi_clf_best_predicted))\n",
    "print('-----------------------------------')\n",
    "print('--- Confusion Matrix ---')\n",
    "print(confusion_matrix(classVector_test, multi_clf_best_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitc modeling\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "C = {\"C\": [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.5, 0.8, 1, 1.5, 5, 10]}\n",
    "log_gs = GridSearchCV(log_clf,C)\n",
    "log_gs.fit(train_data, classVector)\n",
    "\n",
    "log_clf_best = LogisticRegression(C=log_gs.best_params_['C'])\n",
    "log_clf_best.fit(train_data, classVector)\n",
    "log_clf_best_predicted = log_clf_best.predict(test_data)\n",
    "fpr, tpr, _ = metrics.roc_curve(classVector_test, log_clf_best_predicted)\n",
    "\n",
    "print('----- Logistic Regression -----')\n",
    "print (log_gs.best_params_)\n",
    "print (\"Logistic accuracy: {:2.5f}%\".format(accuracy_score(log_clf_best_predicted, classVector_test) * 100))\n",
    "print (\"Logistic accuracy (AUC): {:2.5f}%\".format(metrics.auc(fpr,tpr) * 100))\n",
    "\n",
    "# print confusion matrix to identify mistakes\n",
    "print('--- Confusion Matrix ---')\n",
    "print (confusion_matrix(classVector_test, log_clf_best_predicted))\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
