{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Fake News with Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the GLoVe pre-trained word embedding data set to convert words into N-dimensional vectors. We will use 50 dimensional vectors for now. These vectors were trained on Wikipedia 2014 + Gigaword 5 and includes a 400,000 word vocabulary of uncased words. The file (glove.6B.50d.txt) can be downloaded here: https://nlp.stanford.edu/projects/glove/ . In order to run an LSTM, we will need every article to have the same number of words. Most of the news articles in the Fake News dataset are under 200 words long, including the headline and body. Most of the news articles in the Celebrity data set are under 750 words long. We will begin by capping the article length at 200 words. Articles that are shorter than this, will be padded with zeros (i.e. a random word) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply location of GloVe text file, location of data, and max word length of news article\n",
    "glove_filepath = 'models/embeddings/glove.6B.50d.txt'\n",
    "datapath = 'data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "maxSeqLength = 200\n",
    "numDimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embedding data, and convert it to three useful formats\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    wordsList = []\n",
    "    embeddings = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        wordsList.append(word)\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "        embeddings.append(embedding)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    f.close()\n",
    "    return wordsList, embeddings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# We can access the position of a word in the embedding file using \"wordsList\"\n",
    "# We can access the embedding of a word using \"embeddings\". The position in this will match \"wordlist\".\n",
    "# We can access the embedding of a word using the dictionary \"model\". We will not actually use this, but useful to have.\n",
    "wordsList, embeddings, model = loadGloveModel(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Embed News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "# Function that takes a news article as an input.\n",
    "# It generates a fixed sequences of integers corresponding to the index of the embedding in the embedding lookup\n",
    "# It caps the number of embedded words (i.e. article length) at maxSeqLength\n",
    "# Words that do not exist in GloVe, will be assigned to a random embedding. In this case, the one at position 39999\n",
    "def getArticleMatrix(article):\n",
    "    articleMatrix = np.zeros(maxSeqLength, dtype='int32')\n",
    "    cleanedArticle = cleanArticle(article)\n",
    "    split = cleanedArticle.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        if indexCounter==maxSeqLength:\n",
    "            break\n",
    "        try:\n",
    "            articleMatrix[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            articleMatrix[indexCounter] = 399999 #Vector for unkown words\n",
    "    return articleMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and embed news articles\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = datapath\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data['news_embed'] = getArticleMatrix(result_data['news_all'])\n",
    "                result_data['num_embed_words'] = len(result_data['news_embed'])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>news_all</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_content</th>\n",
       "      <th>news_embed</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_type</th>\n",
       "      <th>num_embed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>polit19.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>FBI investigates computer link between Trump a...</td>\n",
       "      <td>polit</td>\n",
       "      <td>(CNN)Federal investigators and computer scie...</td>\n",
       "      <td>[2419, 20095, 951, 2858, 118, 10468, 5, 443, 3...</td>\n",
       "      <td>FBI investigates computer link between Trump a...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>tech028.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Google steals user location information with a...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Alphabet Inc's Google announced on Wednesday t...</td>\n",
       "      <td>[4361, 13753, 4832, 2044, 419, 17, 7, 12726, 1...</td>\n",
       "      <td>Google steals user location information with a...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>polit34.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Biden: Trump was wiretapped, but not by US    ...</td>\n",
       "      <td>polit</td>\n",
       "      <td>Joe Biden said President Donald Trump was in...</td>\n",
       "      <td>[8725, 10468, 15, 62414, 34, 36, 21, 95, 1984,...</td>\n",
       "      <td>Biden: Trump was wiretapped, but not by US</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu27.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Harvard Law, Moving to Limit Applicant Pool, W...</td>\n",
       "      <td>edu</td>\n",
       "      <td>Harvard Law School, moving to close its door...</td>\n",
       "      <td>[3332, 264, 1233, 4, 2514, 19067, 3216, 43, 36...</td>\n",
       "      <td>Harvard Law, Moving to Limit Applicant Pool, W...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu14.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Microsoft Aims to spread liberalism on the suc...</td>\n",
       "      <td>edu</td>\n",
       "      <td>With the launch of \"Minecraft\"  edition crea...</td>\n",
       "      <td>[2058, 4351, 4, 1635, 22142, 13, 0, 1045, 3, 1...</td>\n",
       "      <td>Microsoft Aims to spread liberalism on the suc...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_name         file_name  is_fake  \\\n",
       "0  fakeNewsDataset  polit19.fake.txt        1   \n",
       "1  fakeNewsDataset  tech028.fake.txt        1   \n",
       "2  fakeNewsDataset  polit34.fake.txt        1   \n",
       "3  fakeNewsDataset    edu27.fake.txt        1   \n",
       "4  fakeNewsDataset    edu14.fake.txt        1   \n",
       "\n",
       "                                            news_all news_category  \\\n",
       "0  FBI investigates computer link between Trump a...         polit   \n",
       "1  Google steals user location information with a...          tech   \n",
       "2  Biden: Trump was wiretapped, but not by US    ...         polit   \n",
       "3  Harvard Law, Moving to Limit Applicant Pool, W...           edu   \n",
       "4  Microsoft Aims to spread liberalism on the suc...           edu   \n",
       "\n",
       "                                        news_content  \\\n",
       "0    (CNN)Federal investigators and computer scie...   \n",
       "1  Alphabet Inc's Google announced on Wednesday t...   \n",
       "2    Joe Biden said President Donald Trump was in...   \n",
       "3    Harvard Law School, moving to close its door...   \n",
       "4    With the launch of \"Minecraft\"  edition crea...   \n",
       "\n",
       "                                          news_embed  \\\n",
       "0  [2419, 20095, 951, 2858, 118, 10468, 5, 443, 3...   \n",
       "1  [4361, 13753, 4832, 2044, 419, 17, 7, 12726, 1...   \n",
       "2  [8725, 10468, 15, 62414, 34, 36, 21, 95, 1984,...   \n",
       "3  [3332, 264, 1233, 4, 2514, 19067, 3216, 43, 36...   \n",
       "4  [2058, 4351, 4, 1635, 22142, 13, 0, 1045, 3, 1...   \n",
       "\n",
       "                                       news_headline news_type  \\\n",
       "0  FBI investigates computer link between Trump a...      fake   \n",
       "1  Google steals user location information with a...      fake   \n",
       "2         Biden: Trump was wiretapped, but not by US      fake   \n",
       "3  Harvard Law, Moving to Limit Applicant Pool, W...      fake   \n",
       "4  Microsoft Aims to spread liberalism on the suc...      fake   \n",
       "\n",
       "   num_embed_words  \n",
       "0              200  \n",
       "1              200  \n",
       "2              200  \n",
       "3              200  \n",
       "4              200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fake news data set\n",
    "# Note: Embedded words used the article title and body. All embeddings should have a length of maxSeqLength.\n",
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confirm embeddings worked properly. The index of the first 5 words should line up!\n",
    "# print(fakenews_df.loc[0]['news_all'])\n",
    "# print(fakenews_df.loc[0]['news_embed'])\n",
    "# print(wordsList.index(\"FBI\".lower()))\n",
    "# print(wordsList.index(\"investigates\".lower()))\n",
    "# print(wordsList.index(\"computer\".lower()))\n",
    "# print(wordsList.index(\"link\".lower()))\n",
    "# print(wordsList.index(\"between\".lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Long Short Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow to build and train a LSTM model which is capable if producing a binary classifier of fake or not fake, for each news artcile.\n",
    "\n",
    "Internal team note: The Oriole LSTM notebook (in /models/LSTM_Classification) has a great explaination of deep learning, recurrent neural networks, LSTMs, word embeddings etc. We can rely heavily on this if we want to explain things in detail in our paper. I recommend reading through that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miketp333/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper functions for training model\n",
    "# def getTrainBatch(ids):\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     for i in range(batchSize):\n",
    "#         if (i % 2 == 0): \n",
    "#             num = randint(1,11499)\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             num = randint(13499,24999)\n",
    "#             labels.append([0,1])\n",
    "#         arr[i] = ids[num-1:num]\n",
    "#     return arr, labels\n",
    "\n",
    "# def getTestBatch(ids):\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     for i in range(batchSize):\n",
    "#         num = randint(11499,13499)\n",
    "#         if (num <= 12499):\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             labels.append([0,1])\n",
    "#         arr[i] = ids[num-1:num]\n",
    "#     return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batchSize = 25\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded word vector lookup. Convert from list to numpy array\n",
    "wordVectors = np.asarray(embeddings)\n",
    "\n",
    "# IDs of news articles\n",
    "newsVectors = np.asarray(fakenews_df['news_embed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slightly different than example workbook.\n",
    "# I case the data as a float in order to get it to work: tf.cast(data,tf.float32)\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, tf.cast(data,tf.float32), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=prediction)) \n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getTrainBatch() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ff5025f660de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m    \u001b[0;31m#Next Batch of reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m    \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m    \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnextBatchLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getTrainBatch() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# # Set up Tensorboard\n",
    "# tf.summary.scalar('Loss', loss)\n",
    "# tf.summary.scalar('Accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "# logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "# # Train model\n",
    "# for i in range(iterations):\n",
    "#    #Next Batch of reviews\n",
    "#    nextBatch, nextBatchLabels = getTrainBatch(newsVectors);\n",
    "#    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "#    #Write summary to Tensorboard\n",
    "#    if (i % 50 == 0):\n",
    "#        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#        writer.add_summary(summary, i)\n",
    "\n",
    "#    #Save the network every 10,000 training iterations\n",
    "#    if (i % 10000 == 0 and i != 0):\n",
    "#        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#        print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
