{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Cleaning and Read Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9' ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def read_perez_dataset(dataset_name):\n",
    "    \n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "    \n",
    "    print(\"Reading dataset\")\n",
    "    result_data_list = []\n",
    "    data_dir = PEREZ_DATASET_PATH\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data_list.append(result_data)\n",
    "                \n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    \n",
    "    df['news_all_clean'] = df['news_all'].apply(lambda a: cleanArticle(a))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['is_fake',\n",
    "                                                               'news_type','file_name'],\n",
    "                                                               axis = 1), \n",
    "                                                        df['is_fake'], \n",
    "                                                        test_size=.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Finished reading dataset\")\n",
    "    return df, X_train, y_train, X_test, y_test\n",
    "\n",
    "def model_report(title, y_test, predictions, predictions_proba):\n",
    "\n",
    "    \"\"\"\n",
    "    Output: Classification report, confusion matrix, and ROC curve\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print(\"---------\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = 'Accuracy: {0}'.format(round(metrics.accuracy_score(y_test, predictions),2))\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, predictions_proba)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Linguistic-based feature creators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ngram & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punct(s):\n",
    "    \"\"\"\n",
    "    Add padding around specified punctuation.\n",
    "    \"\"\"\n",
    "    s = re.sub('([.,!?():])', r' \\1 ', s)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    return s\n",
    "def tfidf_vectorizer_custom(train, test, ngram_range):\n",
    "    \"\"\"\n",
    "    Create a tfidf vectorized set for train and test data that counts punctuation.\n",
    "    Ngram range = (1,3)\n",
    "    \"\"\"\n",
    "    train = train.apply(pad_punct)\n",
    "    test = test.apply(pad_punct)\n",
    "    vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\.|,|\\)|\\(|\\:|\\?|\\\"|\\'\", #pattern keep punctuation in vectorizer\n",
    "                          ngram_range = ngram_range).fit(train)\n",
    "    vocab = vect.vocabulary_.keys()\n",
    "    vocab = sorted(vocab, key=len)\n",
    "    print(\"Traing data info:\")\n",
    "    print('- Vocabulary len:', len(vect.get_feature_names()))\n",
    "    print('- Longest phrase:', max(vect.vocabulary_, key=len))\n",
    "    print('- Smallest 10 phrases:', vocab[0:10])\n",
    "    print('- Sample of features:',np.array(vect.get_feature_names()))\n",
    "    train_vectorized = vect.transform(train)\n",
    "    test_vectorized = vect.transform(test)\n",
    "    return train_vectorized, test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Psycholinguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIWC_counter(df, df_LIWC):\n",
    "    \"\"\"\n",
    "    Inputs: - Data set with field: news_all_clean\n",
    "            - Preprocessed LIWC lookup table\n",
    "    Ouput:  - Data set with 73 additional fields for LIWC count results (normalized over string word count)\n",
    "    \"\"\"\n",
    "    LIWC_vars = train_LIWC.drop('Word', axis = 1).columns.values\n",
    "    for i in LIWC_vars:\n",
    "        df[i] = 0\n",
    "        words = df_LIWC[df_LIWC[i] == 1]['Word']\n",
    "        for a in words:\n",
    "            df[i] = (df[i] + \n",
    "                df.news_all_clean.str.count(str(\" \" + a + \" \"))  + #free floating word\n",
    "                df.news_all_clean.str.count(str(\"^\" + a + \" \")) +  #start word\n",
    "                df.news_all_clean.str.count(str(\" \" + a + \"$\")))   #end word\n",
    "        df[i] = df[i] / df.news_all_clean.str.count(\" \") #normalize over word count\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Read in Data, Define Test/Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "PEREZ_DATASET_PATH = \"../data/fakeNewsDatasets_Perez-Rosas2018\"\n",
    "np.random.seed(RANDOM_SEED)\n",
    "perez_full, train_data, train_labels, test_data, test_labels = read_perez_dataset('fakeNewsDataset')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Run Train/Test Set through LIWC Rented License\n",
    "\n",
    "We rented the LIWC dataset (for 30 days), as seen here: https://store5.esellerate.net/store/checkout/CustomLayout.aspx?s=STR6622550055&pc=&page=OnePageMoreInfo.htm&SkuRefNum=SKU48070077205\n",
    "\n",
    "We downloaded the software, outputted the train/test set, ran the train/test set through the LIWC software, and uploaded the LIWC results.\n",
    "The software marks words to 73 LIWC categories in a binary format (with an 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['news_all'].to_csv(\"train_data_news_all_forLIWC.txt\")\n",
    "test_data['news_all'].to_csv(\"test_data_news_all_forLIWC.txt\")\n",
    "train_LIWC = pd.read_csv(\n",
    "    \"LIWC2015 Results (train_data_news_all_clean_forLIWC).csv\").fillna(0).drop('Unnamed: 74', axis = 1).replace('X',1)\n",
    "test_LIWC = pd.read_csv(\n",
    "    \"LIWC2015 Results (test_data_news_all_clean_forLIWC).csv\").fillna(0).drop('Unnamed: 74', axis = 1).replace('X',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LIWC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_LIWC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Quick Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['news_all_clean'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"full perez size: \", perez_full.shape)\n",
    "print(\"train size: \",train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"news_category\", kind = \"count\", hue=\"is_fake\", data=perez_full)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perez_full.groupby(['news_category','is_fake']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Create Feature-Enriched Training/Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Implement Feature Creating Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ngram & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectorized, test_data_vectorized = tfidf_vectorizer_custom(\n",
    "    train_data['news_all_clean'], test_data['news_all_clean'], ngram_range = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Psycholinguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_LIWC = LIWC_counter(train_data, train_LIWC)\n",
    "test_data_LIWC = LIWC_counter(test_data, test_LIWC)\n",
    "train_data_LIWC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unify All Features in Single Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(sparse, df_to_sparse, df_ignore):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    \"\"\"\n",
    "    return hstack([coo_matrix(df_to_sparse.drop(df_ignore.columns.values, axis = 1)),sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Combine ngram and LIWC\n",
    "train_ngram_LIWC = add_feature(train_data_vectorized, train_data_LIWC, train_data)\n",
    "test_ngram_LIWC = add_feature(test_data_vectorized, test_data_LIWC, test_data)\n",
    "\n",
    "#5. Add news category\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_data['news_category'])\n",
    "train_news_category_vec = vectorizer.transform(train_data['news_category'])\n",
    "test_news_category_vec  = vectorizer.transform(test_data['news_category'])\n",
    "train_final = hstack([train_ngram_LIWC,train_news_category_vec])\n",
    "test_final =  hstack([test_ngram_LIWC,test_news_category_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Classification Models with Automated Machine Learning (TPOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run TPOT Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2,\n",
    "                                   config_dict = \"TPOT sparse\")\n",
    "#pipeline_optimizer.fit(train_final, train_labels) #This takes a couple hours to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Output TPOT Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.export('tpot_ngram_LIWC.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implement TPOT Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=True, criterion=\"gini\", \n",
    "                                           max_features=0.7000000000000001, \n",
    "                                           min_samples_leaf=19, \n",
    "                                           min_samples_split=5, \n",
    "                                           n_estimators=100)\n",
    "\n",
    "\n",
    "exported_pipeline.fit(train_final, train_labels)\n",
    "ORIG_results = exported_pipeline.predict(test_final)\n",
    "ORIG_proba = exported_pipeline.predict_proba(test_final)\n",
    "print(\"accuracy on original test set: \",np.mean(ORIG_results == test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Output TPOT Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
