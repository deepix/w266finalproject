{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def read_perez_dataset(dataset_name):\n",
    "    \n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "    \n",
    "    print(\"Reading dataset\")\n",
    "    result_data_list = []\n",
    "    data_dir = PEREZ_DATASET_PATH\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data_list.append(result_data)\n",
    "                \n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    \n",
    "    df['news_all_clean'] = df['news_all'].apply(lambda a: cleanArticle(a))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['is_fake',\n",
    "                                                               'news_type','file_name'],\n",
    "                                                               axis = 1), \n",
    "                                                        df['is_fake'], \n",
    "                                                        test_size=.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Finished reading dataset\")\n",
    "    return df, X_train, y_train, X_test, y_test\n",
    "\n",
    "def model_report(title, y_test, predictions, predictions_proba):\n",
    "\n",
    "    \"\"\"\n",
    "    Output: Classification report, confusion matrix, and ROC curve\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print(\"---------\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = 'Accuracy: {0}'.format(round(metrics.accuracy_score(y_test, predictions),2))\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, predictions_proba)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Read in Data, Define Test/Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "PEREZ_DATASET_PATH = \"../data/fakeNewsDatasets_Perez-Rosas2018\"\n",
    "np.random.seed(RANDOM_SEED)\n",
    "perez_full, train_data, train_labels, test_data, test_labels = read_perez_dataset('fakeNewsDataset')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Quick Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['news_all_clean'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"full perez size: \", perez_full.shape)\n",
    "print(\"train size: \",train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"news_category\", kind = \"count\", hue=\"is_fake\", data=perez_full)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perez_full.groupby(['news_category','is_fake']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Classification Models with Automated Machine Learning (TPOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define Hstacked Unigram (news_all_clean & news_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_data['news_all_clean'])\n",
    "train_news_all_clean_vec = vectorizer.transform(train_data['news_all_clean'])\n",
    "test_news_all_clean_vec  = vectorizer.transform(test_data['news_all_clean'])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_data['news_category'])\n",
    "train_news_category_vec = vectorizer.transform(train_data['news_category'])\n",
    "test_news_category_vec  = vectorizer.transform(test_data['news_category'])\n",
    "train_vec = hstack([train_news_all_clean_vec,train_news_category_vec])\n",
    "test_vec =  hstack([test_news_all_clean_vec,test_news_category_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run TPOT Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2,\n",
    "                                   config_dict = \"TPOT sparse\")\n",
    "#pipeline_optimizer.fit(train_vec, train_labels) #This takes a couple hours to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Output TPOT Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_optimizer.export('tpot_unigram.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement TPOT Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "features = train_vec\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, train_labels.values, random_state=42)\n",
    "\n",
    "# Score on the training set was:0.6506291953660375\n",
    "exported_pipeline = make_pipeline(\n",
    "    VarianceThreshold(threshold=0.45),\n",
    "    SelectPercentile(score_func=f_classif, percentile=32),\n",
    "    LogisticRegression(C=0.1, dual=True, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "TPOT_results = exported_pipeline.predict(testing_features)\n",
    "ORIG_results = exported_pipeline.predict(test_vec)\n",
    "TPOT_proba = exported_pipeline.predict_proba(testing_features)\n",
    "ORIG_proba = exported_pipeline.predict_proba(test_vec)\n",
    "print(\"accuracy on TPOT test set: \",np.mean(TPOT_results == testing_target))\n",
    "print(\"accuracy on original test set: \",np.mean(ORIG_results == test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Output Unigram Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(\"Original Test Set Report\", test_labels, ORIG_results, ORIG_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
