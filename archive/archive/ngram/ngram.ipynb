{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: n-gram Language Modeling\n",
    "\n",
    "In this part of the assignment, we'll expand on the `SimpleTrigramLM` from the live session demo. We'll add smoothing to improve performance on unseen data, and explore some of the properties of the smoothed model.\n",
    "\n",
    "If you haven't looked over the simple trigram LM in [lm1.ipynb](../../../materials/simple_lm/lm1.ipynb), we recommend familiarizing yourself with it. This assignment will use a very similar set-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on notation\n",
    "\n",
    "\"Primed\" variables, e.g. $c′$. This is just a dummy variable, not necessarily equal to $c$ but usually assumed to be from the same domain. For example: $P(c)$ is the probability of a particular word $c$, while $\\sum_{c′} P(c′)$ is the sum of the probabilities of all possible words $c′$. If not explicitly stated, the domain is usually the domain of the expression inside the sum - so in this case we'd sum for all words $c′ \\in V$ in the vocabulary.\n",
    " \n",
    "- Set notation: $\\{x : f(x) > 0\\}$ means the set of all $x$ where $f(x)>0$. Strictly speaking, we should write $\\{x∈S:f(x)>0\\}$ where $S$ is some other set, but often we omit this when $S$ is implied (such as words (types) in the vocabulary). So $\\{ b′:C_{b′c} > 0 \\}$ is the set of all words (types) $b′$ where the counts $C_{b′c}$ (for some particular word $c$) are greater than zero.\n",
    " \n",
    "- Similar to the above, $\\left| x:f(x)>0 \\right|$ means the number of elements (size) of that set. So $\\left| b′:C_{b′c} > 0 \\right|$ is the number of words $b′$ where the counts $C_{b′c}$ (for some particular word $c$) are greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "from w266_common import utils, vocabulary\n",
    "import ngram_lm, ngram_lm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Implementing the Add-k Model\n",
    "\n",
    "Despite its shortcomings, it's worth implementing an add-k model as a baseline. Unlike the unsmoothed model, we'll be able to get some reasonable (or at least, finite) perplexity numbers which we can compare to the Kneser-Ney model below.\n",
    "\n",
    "We've provided some skeleton code (similar to [lm1.ipynb](../../../materials/simple_lm/lm1.ipynb)) in the `ngram_lm.py` file. In the `AddKTrigramLM` class, implement the following:\n",
    "- `__init__(self, words)`, which computes the necessary corpus statistics $C_{abc}$ and $C_{ab}$.\n",
    "- `next_word_proba(self, word, seq, k)`, which computes $\\hat{P}_k(w\\ |\\ w_{i-1}, w_{i-2})$\n",
    "\n",
    "See the function docstrings and in-line comments for more details. In particular, you may want to use `collections.defaultdict` and `dict.get()` to simplify handling of unknown keys. See [dict_notes.md](dict_notes.md) for a brief overview of how to use these.\n",
    "\n",
    "**Note on keys and word-order:** Convention in the math is to write context in reverse order, as in $P(w\\ |\\ w_{i-1}, w_{i-2})$, but in the code it'll be much easier to write things left-to-right as in normal English: so for the context \"`foo bar ___`\", you'll want to use `(\"foo\", \"bar\")` as a dict key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_totals (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_next_word_proba_k_exists (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_next_word_proba_no_smoothing (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "utils.run_tests(ngram_lm_test, [\"TestAddKTrigramLM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser-Ney Smoothing\n",
    "\n",
    "In this part, we'll explore Kneser-Ney smoothing as a more sophisticated way of estimating unseen probabilities. \n",
    "\n",
    "When building an n-gram model, we're limited by the model order (e.g. trigram, 4-gram, or 5-gram) and how much data is available. Within that, we want to use as much information as possible. Within, say, a trigram context, we can compute a number of different statistics that might be helpful. Let's review a few goals:\n",
    "1. If we don't have good n-gram estimates, we want to back off to (n-1) grams.\n",
    "2. If we back off to (n-1) grams, we should do it \"smoothly\".\n",
    "3. Our counts $C_{abc}$ are probably _overestimates_ for the n-grams we observe (see *held-out reweighting*).\n",
    "4. Type fertilities tell us more about $P(w_{new}\\ |\\ \\text{context})$ than the unigram distribution does.\n",
    "\n",
    "Kneser-Ney smoothing combines all four of these ideas. \n",
    "\n",
    "**Absolute discounting** - which follows from 3. - gives us an easy way to backoff (1. and 2.), by distributing the subtracted probability mass among the backoff distribution $\\tilde{P}(c\\ |\\ b)$.  The amount to redistribute, $\\delta$, is a hyperparameter selected based on a cross-validation set in the usual way, although for this assignment we'll just let $\\delta = 0.75$.\n",
    "\n",
    "$$ P_{ad}(c\\ |\\ b, a) = \\frac{C_{abc} - \\delta}{C_{ab}} + \\alpha_{ab} \\tilde{P}(c\\ |\\  b) $$\n",
    "\n",
    "Where $\\alpha_{ab}$ is a backoff factor, derived from the counts, that guarantees that the probabilities are normalized: $\\sum_{c'} P_{ad}(c'\\ |\\ b, a) = 1$. This definition is recursive: if we let $\\tilde{P}(c\\ |\\  b) = P_{ad}(c\\ |\\ b)$, then the backoff distribution can also back off to even lower n-grams.\n",
    "\n",
    "*Note:* we need the numerator above to positive, so it should actually read $\\max(0, C_{abc} - \\delta)$.\n",
    "\n",
    "**Type fertility** is item 4. Instead of falling back to the unigram distribution at the end, we'll define $\\hat{P}(w)$ as proportional to the type fertility of $w$, or the *number of unique preceding words* $w_{i-1}$.  In the following equation, the word we are estimating the probability of is $c$.  $b'$ are the set of words we've found occurring before $c$ in the training data.\n",
    "\n",
    "$$ \\hat{P}_{tf}(c) \\propto \\left|\\ b' : C_{b'c} > 0\\ \\right| = tf(c)$$\n",
    "\n",
    "In order to make this a valid probability distribution, we need to normalize it with a factor $Z_{tf} = \\sum_{w} tf(w)$, so we have $\\hat{P}_{tf}(w) = \\frac{tf(w)}{Z_{tf}} $\n",
    "\n",
    "### KN Equations\n",
    "\n",
    "Putting it all together, we have our equations for a KN trigram model:\n",
    "\n",
    "$$ P_{kn}(c\\ |\\ b, a) = \\frac{\\max(0, C_{abc} - \\delta)}{C_{ab}} + \\alpha_{ab} P_{kn}(c\\ |\\  b) $$\n",
    "where the bigram backoff is:\n",
    "$$ P_{kn}(c\\ |\\ b) = \\frac{\\max(0, C_{bc} - \\delta)}{C_{b}} + \\alpha_{b} P_{kn}(c) $$\n",
    "and the unigram (type fertility) backoff is:\n",
    "$$ P_{kn}(c) = \\frac{tf(c)}{Z_{tf}} \\quad \\text{where} \\quad tf(c) = \\left|\\ b' : C_{b'c} > 0\\ \\right| $$\n",
    "\n",
    "Note that there is only one free parameter in in the above equations: $\\delta$. You'll compute $\\alpha_{ab}$ and $\\alpha_{b}$ in the exercise below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Implementing the KN Model\n",
    "\n",
    "Implement the `KNTrigramLM` in `ngram_lm.py`. As with the add-k model, we've provided some starter code for you; you need only fill in the marked blocks. `KNTrigramLM` also conforms to the exact same interface as the add-k model.\n",
    "\n",
    "You should:\n",
    "- Finish the implementation of `__init__(self, words)` to compute the necessary quantities\n",
    "- Implement the `kn_interp(...)` function, which interpolates between n-gram counts and a backoff distribution according to the absolute discounting rule (see definitions of $P_{kn}(c\\ |\\ a, b)$ and $P_{kn}(c\\ |\\ b)$). You'll need your definition of $\\alpha$ from (c).1. here.\n",
    "\n",
    "As before, see the function docstrings and in-line comments for more details.\n",
    "\n",
    "When you're done implementing it, run the cells below to train your model, sample some sentences, and evaluate your model on the dev set. Then jump to Part (e) for a few last questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_nnz (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_context_totals (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_kn_interp (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_next_word_proba (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestKNTrigramLM)\n",
      "Don't allow modifications to the LM during inference. ... ok\n",
      "test_type_contexts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_type_fertility (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_z_tf (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.011s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "utils.run_tests(ngram_lm_test, [\"TestKNTrigramLM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your Model\n",
    "\n",
    "The same code below can be used with either model; in the cell where it says \"Select your Model\", you can choose the add-k model or the KN model.\n",
    "\n",
    "## Loading & Preprocessing\n",
    "Once again, we'll build our model on the Brown corpus. We'll do an 80/20 train/test split, and preprocess words by lowercasing and replacing digits with `DG` (so `2016` becomes `DGDGDGDG`).\n",
    "\n",
    "In a slight departure from the `lm1.ipynb` demo, we'll restrict the vocabulary to 40000 words. This way, a small fraction of the *training* data will be mapped to `<unk>` tokens, and the model can learn n-gram probabilities that include `<unk>` for prediction on the test set. (If we interpret `<unk>` as meaning \"rare word\", then this is somewhat plausible as a way to infer things about the class of rare words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/miketp333/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (979,646 tokens)\n",
      "Test set: 11,468 sentences (181,546 tokens)\n",
      "Train set vocabulary: 30,000 words\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download('brown'))  # Make sure we have the data.\n",
    "corpus = nltk.corpus.brown\n",
    "V = 30000\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "# Build vocabulary only on the training set.\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "print(\"Train set vocabulary: {:,} words\".format(vocab.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our smoothed models will also be trigram models, so for convenience we'll also prepend *two* `<s>` markers. (We could avoid this, but then we'd need special handling for the first token of each sentence.)\n",
    "\n",
    "To make it easier to work with, we'll take the list of tokens as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: \n",
      " array(['<s>', '<s>', 'the', 'fulton', 'county', 'grand', 'jury', 'said',\n",
      "       'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent',\n",
      "       'primary', 'election', 'produced', '``', 'no', 'evidence'],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([u\"<s>\", u\"<s>\"] + s + [u\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "print(\"Sample data: \\n\", repr(train_tokens[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your model\n",
    "\n",
    "Select either `AddKTrigramLM` or `KNTrigramLM` in the cell below. If switching models, you only need to re-run the cells below here - no need to re-run the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 6.45 s\n",
      "=== N-gram Language Model stats ===\n",
      "  30,000 unique 1-grams\n",
      " 357,896 unique 2-grams\n",
      " 732,467 unique 3-grams\n",
      "Optimal memory usage (counts only): 24.21 MB\n"
     ]
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "\n",
    "# Uncomment the line below for the model you want to run.\n",
    "#Model = ngram_lm.AddKTrigramLM\n",
    "Model = ngram_lm.KNTrigramLM\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Building trigram LM... \", end=\"\")\n",
    "lm = Model(train_tokens)\n",
    "print(\"done in {:.02f} s\".format(time.time() - t0))\n",
    "lm.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `params` to change the smoothing factor. `AddKTrigramLM` will ignore the value of `delta`, and `KNTrigramLM` will ignore `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> to have lived through it as a rule twelfth gardening advertising uncle morse and possibly celebrate . </s>\n",
      "[17 tokens; log P(seq): -144.32]\n",
      "\n",
      "<s> <s> and many unique in the double this will be was the staying and wanted to is easier festivals will mathematics\n",
      "[20 tokens; log P(seq): -177.75]\n",
      "\n",
      "<s> <s> <s> but theirs . </s>\n",
      "[3 tokens; log P(seq): -19.32]\n",
      "\n",
      "<s> <s> his southeast diminishing undertake ; </s>\n",
      "[5 tokens; log P(seq): -63.05]\n",
      "\n",
      "<s> <s> it was the ambassador for example , and ways of suffering '' suggestive enough that struggling virginian , and girls\n",
      "[20 tokens; log P(seq): -155.02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(lm.sample_next(seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print(\" \".join(seq))\n",
    "    print(\"[{1:d} tokens; log P(seq): {0:.02f}]\\n\".format(*lm.score_seq(seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring on Held-Out Data\n",
    "\n",
    "Your KN model should get a perplexity of around 280 on the dev set with $\\delta = 0.75$. The add-k smoothing model will perform... somewhat worse :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 17.14\n",
      "Test perplexity: 283.93\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = lm.score_seq(train_tokens)\n",
    "print(\"Train perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))\n",
    "\n",
    "log_p_data, num_real_tokens = lm.score_seq(test_tokens)\n",
    "print(\"Test perplexity: {:.02f}\".format(2**(-1*log_p_data/num_real_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
