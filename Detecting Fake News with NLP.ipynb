{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Fake News with Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the GLoVe pre-trained word embedding data set to convert words into N-dimensional vectors. We will use 50 dimensional vectors for now. These vectors were trained on Wikipedia 2014 + Gigaword 5 and includes a 400,000 word vocabulary of uncased words. The file (glove.6B.50d.txt) can be downloaded here: https://nlp.stanford.edu/projects/glove/ . In order to run an LSTM, we will need every article to have the same number of words. Most of the news articles in the Fake News dataset are under 200 words long, including the headline and body. Most of the news articles in the Celebrity data set are under 750 words long. We will begin by capping the article length at 200 words. Articles that are shorter than this, will be padded with zeros (i.e. a random word) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply location of GloVe text file, location of data, and max word length of news article\n",
    "glove_filepath = 'models/embeddings/glove.6B.50d.txt'\n",
    "datapath = 'data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "maxSeqLength = 200\n",
    "numDimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embedding data, and convert it to three useful formats\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    wordsList = []\n",
    "    embeddings = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        wordsList.append(word)\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "        embeddings.append(embedding)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    f.close()\n",
    "    return wordsList, embeddings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the position of a word in the embedding file using \"wordsList\"\n",
    "# We can access the embedding of a word using \"embeddings\". The position in this will match \"wordlist\".\n",
    "# We can access the embedding of a word using the dictionary \"model\". We will not actually use this, but useful to have.\n",
    "wordsList, embeddings, model = loadGloveModel(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Embed News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "# Function that takes a news article as an input.\n",
    "# It generates a fixed sequences of integers corresponding to the index of the embedding in the embedding lookup\n",
    "# It caps the number of embedded words (i.e. article length) at maxSeqLength\n",
    "# Words that do not exist in GloVe, will be assigned to a random embedding. In this case, the one at position 39999\n",
    "def getArticleMatrix(article):\n",
    "    articleMatrix = np.zeros(maxSeqLength, dtype='int32')\n",
    "    cleanedArticle = cleanArticle(article)\n",
    "    split = cleanedArticle.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        if indexCounter==maxSeqLength:\n",
    "            break\n",
    "        try:\n",
    "            articleMatrix[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            articleMatrix[indexCounter] = 399999 #Vector for unkown words\n",
    "    return articleMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and embed news articles\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = datapath\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data['news_embed'] = getArticleMatrix(result_data['news_all'])\n",
    "                result_data['num_embed_words'] = len(result_data['news_embed'])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fake news data set\n",
    "# Note: Embedded words used the article title and body. All embeddings should have a length of maxSeqLength.\n",
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confirm embeddings worked properly. The index of the first 5 words should line up!\n",
    "# print(fakenews_df.loc[0]['news_all'])\n",
    "# print(fakenews_df.loc[0]['news_embed'])\n",
    "# print(wordsList.index(\"FBI\".lower()))\n",
    "# print(wordsList.index(\"investigates\".lower()))\n",
    "# print(wordsList.index(\"computer\".lower()))\n",
    "# print(wordsList.index(\"link\".lower()))\n",
    "# print(wordsList.index(\"between\".lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Long Short Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow to build and train a LSTM model which is capable if producing a binary classifier of fake or not fake, for each news artcile.\n",
    "\n",
    "Internal team note: The Oriole LSTM notebook (in /models/LSTM_Classification) has a great explaination of deep learning, recurrent neural networks, LSTMs, word embeddings etc. We can rely heavily on this if we want to explain things in detail in our paper. I recommend reading through that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate news df to use\n",
    "news_df = fakenews_df\n",
    "\n",
    "# Embedded word vector lookup. Convert from list to numpy array\n",
    "wordVectors = np.asarray(embeddings)\n",
    "\n",
    "# Split news articles and classification into test and train sets\n",
    "newsVectors, newsVectors_test, classVector, classVector_test = \\\n",
    "    train_test_split(news_df['news_embed'],\n",
    "                     news_df['is_fake'],\n",
    "                     test_size = .2,\n",
    "                     random_state = 1)\n",
    "\n",
    "newsVectors, newsVectors_test, classVector, classVector_test = \\\n",
    "    np.asarray(newsVectors), \\\n",
    "    np.asarray(newsVectors_test), \\\n",
    "    np.asarray(classVector), \\\n",
    "    np.asarray(classVector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training model\n",
    "# The label is converted to 2 dimensions. First column is flagged with 1 if fake. Second column is 1 if real.\n",
    "# since our output is a 2 way classification\n",
    "def getTrainBatchNew(ids, labels, batch_num):\n",
    "    start_idx = batch_num * batchSize\n",
    "    end_idx = start_idx + batchSize\n",
    "    out_array = np.zeros([batchSize, maxSeqLength])\n",
    "    out_labels = []\n",
    "    out_array_idx = 0\n",
    "    for i in range(start_idx, end_idx):\n",
    "        if labels[i] == 0:\n",
    "            out_labels.append([0, 1])\n",
    "        else:\n",
    "            out_labels.append([1, 0])\n",
    "        out_array[out_array_idx] = ids[i]\n",
    "        out_array_idx += 1\n",
    "    return np.asarray(out_array), np.asarray(out_labels)\n",
    "\n",
    "def getTrainBatchNew2(ids, label):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # Get indexes of real and fake news and shuffle them\n",
    "    real = list(np.where(label==0)[0])\n",
    "    fake = list(np.where(label==1)[0])\n",
    "    random.shuffle(real)\n",
    "    random.shuffle(fake)\n",
    "    \n",
    "    # If batch size an even number. Split evenly fake and real\n",
    "    if (batchSize % 2 == 0):\n",
    "        comb = real[0:int(batchSize/2)] + fake[0:int(batchSize/2)]\n",
    "        # Repeat the labels\n",
    "        labels = [[0,1]] * int(batchSize/2) + [[1,0]] * int(batchSize/2)\n",
    "    # If batch size an odd number. Split evenly fake and real, and add 1 fake\n",
    "    else:\n",
    "        comb = real[0:int(batchSize/2)] + fake[0:int(batchSize/2)]\n",
    "        comb = comb + fake[int(batchSize/2):int(batchSize/2)+1]\n",
    "        labels = [[0,1]] * int(batchSize/2) + [[1,0]] * int(batchSize/2)\n",
    "        labels = labels +  [[1,0]]\n",
    "    \n",
    "    for i in range(len(comb)):\n",
    "        arr[i] = ids[comb]\n",
    "          \n",
    "    return arr, np.asarray(labels)\n",
    "\n",
    "def getTrainBatch(ids, label):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        \n",
    "        # Select an even number of fake and real news for every batch\n",
    "        if (i % 2 == 0):\n",
    "            # Randomly select from real news\n",
    "            num = random.choice(list(np.where(label==0)[0]))\n",
    "            arr[i] = ids[num]\n",
    "            labels.append([0,1])\n",
    "        else:\n",
    "            # Randomly select from fake news\n",
    "            num = random.choice(list(np.where(label==1)[0]))\n",
    "            arr[i] = ids[num]\n",
    "            labels.append([1,0])\n",
    "       \n",
    "    return arr, np.asarray(labels)\n",
    "\n",
    "# Use all test data. Make sure batch size = length(test data) because I did not make batch size dynamic\n",
    "def getTestBatch(ids, label):\n",
    "    labels = []\n",
    "    num_test = len(label)\n",
    "    arr = np.zeros([num_test, maxSeqLength])\n",
    "    for i in range(num_test):\n",
    "        arr[i] = ids[i]\n",
    "        if label[i] == 0:\n",
    "            labels.append([0,1])\n",
    "        else:\n",
    "            labels.append([1,0])\n",
    "            \n",
    "    return arr, np.asarray(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - We can turn up the batchsize and iterations when we want to train much more\n",
    "# Making batchSize equal to length of the test set, so when obtain results it uses entire test set\n",
    "batchSize = len(classVector_test)\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slightly different than example workbook.\n",
    "# I cast the data as a float in order to get it to work: tf.cast(data,tf.float32)\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, forget_bias=0.0)\n",
    "# Add dropout or not?\n",
    "# lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, final_h_ = tf.nn.dynamic_rnn(lstmCell, tf.cast(data,tf.float32), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.0, shape=[numClasses]))\n",
    "# Transpose rows and columns (0-->1, 1-->0, 2-->2)\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=prediction)) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pdb\n",
    "\n",
    "# Takes approximately 1 hour to run. (4 vCPUs, 15 GB memory)\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Set up Tensorboard\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "# Train model\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "#     if len(newsVectors) % int(batchSize) == 0:\n",
    "#         num_batches = int(len(newsVectors) / batchSize)\n",
    "#     else:\n",
    "#         # TODO: fix this to pad\n",
    "#         num_batches = int(len(newsVectors) // batchSize)\n",
    "#     for b in range(num_batches):\n",
    "#         nextBatch, nextBatchLabels = getTrainBatchNew(newsVectors, classVector, b)\n",
    "#         sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    nextBatch, nextBatchLabels = getTrainBatchNew2(newsVectors, classVector)\n",
    "    \n",
    "    # Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations, or on last iteration\n",
    "    if ((i % 10000 == 0 and i != 0) or i == iterations - 1):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch(newsVectors_test, classVector_test);\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View confusion matrix of one batch\n",
    "nextBatch, nextBatchLabels = getTestBatch(newsVectors_test, classVector_test)\n",
    "predictions = sess.run(prediction, {input_data: nextBatch})\n",
    "predictions = [p[0] > p[1] for p in predictions]\n",
    "classVector_test\n",
    "print(confusion_matrix(classVector_test, predictions))\n",
    "print(\"Accuracy:\", sum(classVector_test==predictions) / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
