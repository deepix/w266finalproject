{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Fake News with Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the GLoVe pre-trained word embedding data set to convert words into N-dimensional vectors. We will use 50 dimensional vectors for now. These vectors were trained on Wikipedia 2014 + Gigaword 5 and includes a 400,000 word vocabulary of uncased words. The file (glove.6B.50d.txt) can be downloaded here: https://nlp.stanford.edu/projects/glove/ . In order to run an LSTM, we will need every article to have the same number of words. Most of the news articles in the Fake News dataset are under 200 words long, including the headline and body. Most of the news articles in the Celebrity data set are under 750 words long. We will begin by capping the article length at 200 words. Articles that are shorter than this, will be padded with zeros (i.e. a random word) at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply location of GloVe text file, location of data, and max word length of news article\n",
    "glove_filepath = 'models/embeddings/glove.6B.50d.txt'\n",
    "datapath = 'data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "maxSeqLength = 200\n",
    "numDimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embedding data, and convert it to three useful formats\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    wordsList = []\n",
    "    embeddings = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        wordsList.append(word)\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "        embeddings.append(embedding)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    f.close()\n",
    "    return wordsList, embeddings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# We can access the position of a word in the embedding file using \"wordsList\"\n",
    "# We can access the embedding of a word using \"embeddings\". The position in this will match \"wordlist\".\n",
    "# We can access the embedding of a word using the dictionary \"model\". We will not actually use this, but useful to have.\n",
    "wordsList, embeddings, model = loadGloveModel(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Embed News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "# Function that takes a news article as an input.\n",
    "# It generates a fixed sequences of integers corresponding to the index of the embedding in the embedding lookup\n",
    "# It caps the number of embedded words (i.e. article length) at maxSeqLength\n",
    "# Words that do not exist in GloVe, will be assigned to a random embedding. In this case, the one at position 39999\n",
    "def getArticleMatrix(article):\n",
    "    articleMatrix = np.zeros(maxSeqLength, dtype='int32')\n",
    "    cleanedArticle = cleanArticle(article)\n",
    "    split = cleanedArticle.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        if indexCounter==maxSeqLength:\n",
    "            break\n",
    "        try:\n",
    "            articleMatrix[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            articleMatrix[indexCounter] = 399999 #Vector for unkown words\n",
    "    return articleMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and embed news articles\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = datapath\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data['news_embed'] = getArticleMatrix(result_data['news_all'])\n",
    "                result_data['num_embed_words'] = len(result_data['news_embed'])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>news_all</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_content</th>\n",
       "      <th>news_embed</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_type</th>\n",
       "      <th>num_embed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>entmt04.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>He is  not very familiar with winning awards f...</td>\n",
       "      <td>entmt</td>\n",
       "      <td>He is  not very familiar with winning awards f...</td>\n",
       "      <td>[18, 14, 36, 191, 3478, 17, 877, 1542, 10, 26,...</td>\n",
       "      <td></td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu39.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Companies and Colleges can get the U.S. Workin...</td>\n",
       "      <td>edu</td>\n",
       "      <td>The U.S. Labor Department announced last Fri...</td>\n",
       "      <td>[337, 5, 4759, 86, 169, 0, 95, 500, 378, 0, 95...</td>\n",
       "      <td>Companies and Colleges can get the U.S. Workin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>tech031.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram adds futuristic authentication, faci...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Instagram has added on some futuristic feature...</td>\n",
       "      <td>[109262, 2144, 20746, 30854, 12662, 3275, 5, 3...</td>\n",
       "      <td>Instagram adds futuristic authentication, faci...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>tech024.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Toyota sues Microsoft for contract breach     ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Automobile manufacturer Toyota had signed ...</td>\n",
       "      <td>[3951, 25115, 2058, 10, 953, 7218, 6190, 5023,...</td>\n",
       "      <td>Toyota sues Microsoft for contract breach</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>polit20.fake.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>In Second Debate, Donald Trump and Hillary Cli...</td>\n",
       "      <td>polit</td>\n",
       "      <td>Hillary Clinton and Donald Trump clashed in ...</td>\n",
       "      <td>[6, 126, 1422, 3907, 10468, 5, 4539, 443, 3008...</td>\n",
       "      <td>In Second Debate, Donald Trump and Hillary Cli...</td>\n",
       "      <td>fake</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_name         file_name  is_fake  \\\n",
       "0  fakeNewsDataset  entmt04.fake.txt        1   \n",
       "1  fakeNewsDataset    edu39.fake.txt        1   \n",
       "2  fakeNewsDataset  tech031.fake.txt        1   \n",
       "3  fakeNewsDataset  tech024.fake.txt        1   \n",
       "4  fakeNewsDataset  polit20.fake.txt        1   \n",
       "\n",
       "                                            news_all news_category  \\\n",
       "0  He is  not very familiar with winning awards f...         entmt   \n",
       "1  Companies and Colleges can get the U.S. Workin...           edu   \n",
       "2  Instagram adds futuristic authentication, faci...          tech   \n",
       "3  Toyota sues Microsoft for contract breach     ...          tech   \n",
       "4  In Second Debate, Donald Trump and Hillary Cli...         polit   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  He is  not very familiar with winning awards f...   \n",
       "1    The U.S. Labor Department announced last Fri...   \n",
       "2  Instagram has added on some futuristic feature...   \n",
       "3      Automobile manufacturer Toyota had signed ...   \n",
       "4    Hillary Clinton and Donald Trump clashed in ...   \n",
       "\n",
       "                                          news_embed  \\\n",
       "0  [18, 14, 36, 191, 3478, 17, 877, 1542, 10, 26,...   \n",
       "1  [337, 5, 4759, 86, 169, 0, 95, 500, 378, 0, 95...   \n",
       "2  [109262, 2144, 20746, 30854, 12662, 3275, 5, 3...   \n",
       "3  [3951, 25115, 2058, 10, 953, 7218, 6190, 5023,...   \n",
       "4  [6, 126, 1422, 3907, 10468, 5, 4539, 443, 3008...   \n",
       "\n",
       "                                       news_headline news_type  \\\n",
       "0                                                         fake   \n",
       "1  Companies and Colleges can get the U.S. Workin...      fake   \n",
       "2  Instagram adds futuristic authentication, faci...      fake   \n",
       "3         Toyota sues Microsoft for contract breach       fake   \n",
       "4  In Second Debate, Donald Trump and Hillary Cli...      fake   \n",
       "\n",
       "   num_embed_words  \n",
       "0              200  \n",
       "1              200  \n",
       "2              200  \n",
       "3              200  \n",
       "4              200  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fake news data set\n",
    "# Note: Embedded words used the article title and body. All embeddings should have a length of maxSeqLength.\n",
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confirm embeddings worked properly. The index of the first 5 words should line up!\n",
    "# print(fakenews_df.loc[0]['news_all'])\n",
    "# print(fakenews_df.loc[0]['news_embed'])\n",
    "# print(wordsList.index(\"FBI\".lower()))\n",
    "# print(wordsList.index(\"investigates\".lower()))\n",
    "# print(wordsList.index(\"computer\".lower()))\n",
    "# print(wordsList.index(\"link\".lower()))\n",
    "# print(wordsList.index(\"between\".lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Long Short Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow to build and train a LSTM model which is capable if producing a binary classifier of fake or not fake, for each news artcile.\n",
    "\n",
    "Internal team note: The Oriole LSTM notebook (in /models/LSTM_Classification) has a great explaination of deep learning, recurrent neural networks, LSTMs, word embeddings etc. We can rely heavily on this if we want to explain things in detail in our paper. I recommend reading through that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miketp333/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate news df to use\n",
    "news_df = fakenews_df\n",
    "\n",
    "# Embedded word vector lookup. Convert from list to numpy array\n",
    "wordVectors = np.asarray(embeddings)\n",
    "\n",
    "# Split news articles and classification into test and train sets\n",
    "newsVectors, newsVectors_test, classVector, classVector_test = \\\n",
    "    train_test_split(news_df['news_embed'],\n",
    "                     news_df['is_fake'],\n",
    "                     test_size = .2,\n",
    "                     random_state = 1)\n",
    "\n",
    "newsVectors, newsVectors_test, classVector, classVector_test = \\\n",
    "    np.asarray(newsVectors), \\\n",
    "    np.asarray(newsVectors_test), \\\n",
    "    np.asarray(classVector), \\\n",
    "    np.asarray(classVector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training model\n",
    "# The label is converted to 2 dimensions. First column is flagged with 1 if fake. Second column is 1 if real.\n",
    "# since our output is a 2 way classification\n",
    "\n",
    "# def getTrainBatchOld2(ids, labels, batch_num):\n",
    "#     start_idx = batch_num * batchSize\n",
    "#     end_idx = start_idx + batchSize\n",
    "#     out_array = np.zeros([batchSize, maxSeqLength])\n",
    "#     out_labels = []\n",
    "#     out_array_idx = 0\n",
    "#     for i in range(start_idx, end_idx):\n",
    "#         if labels[i] == 0:\n",
    "#             out_labels.append([0, 1])\n",
    "#         else:\n",
    "#             out_labels.append([1, 0])\n",
    "#         out_array[out_array_idx] = ids[i]\n",
    "#         out_array_idx += 1\n",
    "#     return np.asarray(out_array), np.asarray(out_labels)\n",
    "\n",
    "def getTrainBatch(ids, label):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # Get indexes of real and fake news and shuffle them\n",
    "    real = list(np.where(label==0)[0])\n",
    "    fake = list(np.where(label==1)[0])\n",
    "    random.shuffle(real)\n",
    "    random.shuffle(fake)\n",
    "    \n",
    "    # If batch size an even number. Split evenly fake and real\n",
    "    if (batchSize % 2 == 0):\n",
    "        comb = real[0:int(batchSize/2)] + fake[0:int(batchSize/2)]\n",
    "        # Repeat the labels\n",
    "        labels = [[0,1]] * int(batchSize/2) + [[1,0]] * int(batchSize/2)\n",
    "    # If batch size an odd number. Split evenly fake and real, and add 1 fake\n",
    "    else:\n",
    "        comb = real[0:int(batchSize/2)] + fake[0:int(batchSize/2)]\n",
    "        comb = comb + fake[int(batchSize/2):int(batchSize/2)+1]\n",
    "        labels = [[0,1]] * int(batchSize/2) + [[1,0]] * int(batchSize/2)\n",
    "        labels = labels +  [[1,0]]\n",
    "    \n",
    "    for i in range(len(comb)):\n",
    "        arr[i] = ids[comb[i]]\n",
    "          \n",
    "    return arr, np.asarray(labels)\n",
    "\n",
    "# def getTrainBatchOld1(ids, label):\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     for i in range(batchSize):\n",
    "        \n",
    "#         # Select an even number of fake and real news for every batch\n",
    "#         if (i % 2 == 0):\n",
    "#             # Randomly select from real news\n",
    "#             num = random.choice(list(np.where(label==0)[0]))\n",
    "#             arr[i] = ids[num]\n",
    "#             labels.append([0,1])\n",
    "#         else:\n",
    "#             # Randomly select from fake news\n",
    "#             num = random.choice(list(np.where(label==1)[0]))\n",
    "#             arr[i] = ids[num]\n",
    "#             labels.append([1,0])\n",
    "       \n",
    "#     return arr, np.asarray(labels)\n",
    "\n",
    "# Use all test data. Make sure batch size = length(test data) because I did not make batch size dynamic\n",
    "def getTestBatch(ids, label):\n",
    "    labels = []\n",
    "    num_test = len(label)\n",
    "    arr = np.zeros([num_test, maxSeqLength])\n",
    "    for i in range(num_test):\n",
    "        arr[i] = ids[i]\n",
    "        if label[i] == 0:\n",
    "            labels.append([0,1])\n",
    "        else:\n",
    "            labels.append([1,0])\n",
    "            \n",
    "    return arr, np.asarray(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - We can turn up the batchsize and iterations when we want to train much more\n",
    "# Making batchSize equal to length of the test set, so when obtain results it uses entire test set\n",
    "batchSize = len(classVector_test)\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slightly different than example workbook.\n",
    "# I cast the data as a float in order to get it to work: tf.cast(data,tf.float32)\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, forget_bias=0.0)\n",
    "# Add dropout or not?\n",
    "# lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, final_h_ = tf.nn.dynamic_rnn(lstmCell, tf.cast(data,tf.float32), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.0, shape=[numClasses]))\n",
    "# Transpose rows and columns (0-->1, 1-->0, 2-->2)\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-f39c2a79afe1>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=prediction)) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-19\n"
     ]
    }
   ],
   "source": [
    "# %pdb\n",
    "\n",
    "# Takes approximately 1 hour to run. (4 vCPUs, 15 GB memory)\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Set up Tensorboard\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "# Train model\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "#     if len(newsVectors) % int(batchSize) == 0:\n",
    "#         num_batches = int(len(newsVectors) / batchSize)\n",
    "#     else:\n",
    "#         # TODO: fix this to pad\n",
    "#         num_batches = int(len(newsVectors) // batchSize)\n",
    "#     for b in range(num_batches):\n",
    "#         nextBatch, nextBatchLabels = getTrainBatchNew(newsVectors, classVector, b)\n",
    "#         sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    nextBatch, nextBatchLabels = getTrainBatch(newsVectors, classVector)\n",
    "    \n",
    "    # Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations, or on last iteration\n",
    "    if ((i % 10000 == 0 and i != 0) or i == iterations - 1):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-19\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 59.375\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch(newsVectors_test, classVector_test);\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  0]\n",
      " [39  1]]\n",
      "Accuracy: 0.59375\n"
     ]
    }
   ],
   "source": [
    "# View confusion matrix of one batch\n",
    "nextBatch, nextBatchLabels = getTestBatch(newsVectors_test, classVector_test)\n",
    "predictions = sess.run(prediction, {input_data: nextBatch})\n",
    "predictions = [p[0] > p[1] for p in predictions]\n",
    "classVector_test\n",
    "print(confusion_matrix(classVector_test, predictions))\n",
    "print(\"Accuracy:\", sum(classVector_test==predictions) / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
