{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and using embeddings\n",
    "\n",
    "We will use GLoVe pre-trained data set to convert some sentences into N-dimensional vectors.\n",
    "\n",
    "First, we will use the \"gensim\" package to load the dataset into an easily consumable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe vectors\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def load_glove_model_v1(dim=50):\n",
    "    \"\"\"Load a Glove dataset into a Pandas dataframe\n",
    "    Returns: embedding\"\"\"\n",
    "    glove_data_file = 'glove.6B.%dd.txt' % dim\n",
    "    embedding_df = pd.read_table(glove_data_file, sep=\" \", index_col=0, \n",
    "                                 header=None, quoting=csv.QUOTE_NONE,\n",
    "                                 na_values=None, keep_default_na=False)\n",
    "    return embedding_df\n",
    "\n",
    "def load_glove_model_v2(dim=50):\n",
    "    \"\"\"Load a Glove model into a gensim model, converting it\n",
    "    into word2vec if necessary.\n",
    "    Adapted from: https://stackoverflow.com/a/47465278\n",
    "    \"\"\"\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    from gensim.models.keyedvectors import KeyedVectors\n",
    "    from pathlib import Path\n",
    "\n",
    "    glove_data_file = 'glove.6B.%dd.txt' % dim\n",
    "    word2vec_output_file = '%s.w2v' % glove_data_file\n",
    "\n",
    "    if not Path(word2vec_output_file).exists():\n",
    "        glove2word2vec(glove_input_file=glove_data_file, word2vec_output_file=word2vec_output_file)\n",
    "    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    return glove_model\n",
    "\n",
    "# We will use v2, because it is more versatile\n",
    "dim = 50\n",
    "model = load_glove_model_v2(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the embedding matrix for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an embedding matrix out of embedding vectors\n",
    "embedding_matrix = np.zeros((len(model.vocab), dim))\n",
    "for i in range(len(model.vocab)):\n",
    "    embedding_vector = model[model.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run this embedding model on a sample sentence within TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/\n",
    "\n",
    "# Print a lookup for a sample sentence with word IDs [1, 5, 10]\n",
    "import tensorflow  as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sentences = tf.placeholder(tf.int32, shape=[None,None])\n",
    "\n",
    "saved_embeddings = tf.constant(embedding_matrix)\n",
    "embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "embedding_lookup = tf.nn.embedding_lookup(embedding, sentences)\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(embedding_lookup,\n",
    "                       feed_dict={sentences:[[1, 5, 10]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works.  Next, we will convert our dataset into a form suitable for this training.  We should also experiment with [GoogleNews word2vec dataset](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/), because that is more relevant to a Fake News project.\n",
    "\n",
    "### Look up embeddings for our dataset\n",
    "\n",
    "We first load our data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def get_word_list(article):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    words_only = re.sub(strip_special_chars, \"\", article.lower())\n",
    "    return words_only.split()\n",
    "\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = '../../data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data['news_word_list'] = get_word_list(news_content_data)\n",
    "                result_data['num_words'] = len(result_data['news_word_list'])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look up each word in the vocabulary of our embedding model.  If found, we note the index within the vocabulary.\n",
    "\n",
    "To make this fast, we use the vectorized \"apply\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(word_list):\n",
    "    word_index_list = []\n",
    "    for word in word_list:\n",
    "        if word in model.vocab:\n",
    "            word_index_list.append(model.vocab[word].index)\n",
    "    return word_index_list\n",
    "fakenews_df['embedding_ids'] = fakenews_df['news_word_list'].apply(lambda x: word_to_id(x))\n",
    "fakenews_df['num_embedding_ids'] = fakenews_df['embedding_ids'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to print the embeddings for one of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_article = fakenews_df['embedding_ids'].tolist()[0]\n",
    "print(sample_article)\n",
    "\n",
    "import tensorflow  as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sentences = tf.placeholder(tf.int32, shape=[None,None])\n",
    "\n",
    "saved_embeddings = tf.constant(embedding_matrix)\n",
    "embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "embedding_lookup = tf.nn.embedding_lookup(embedding, sentences)\n",
    "\n",
    "# Broken at the moment\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(embedding_lookup,\n",
    "                       feed_dict={sentences: [sample_article]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
