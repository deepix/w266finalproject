{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and using embeddings\n",
    "\n",
    "We will use GLoVe pre-trained data set to convert some sentences into N-dimensional vectors.\n",
    "\n",
    "First, we will use the \"gensim\" package to load the dataset into an easily consumable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, some imports\n",
    "\n",
    "from importlib import reload\n",
    "import shutil\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow  as tf\n",
    "\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import rnnlm_deepak as rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe vectors\n",
    "\n",
    "def load_glove_model_v1(dim=50):\n",
    "    \"\"\"Load a Glove dataset into a Pandas dataframe\n",
    "    Returns: embedding\"\"\"\n",
    "    glove_data_file = 'glove.6B.%dd.txt' % dim\n",
    "    embedding_df = pd.read_table(glove_data_file, sep=\" \", index_col=0, \n",
    "                                 header=None, quoting=csv.QUOTE_NONE,\n",
    "                                 na_values=None, keep_default_na=False)\n",
    "    return embedding_df\n",
    "\n",
    "def load_glove_model_v2(dim=50):\n",
    "    \"\"\"Load a Glove model into a gensim model, converting it\n",
    "    into word2vec if necessary.\n",
    "    Adapted from: https://stackoverflow.com/a/47465278\n",
    "    \"\"\"\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    from gensim.models.keyedvectors import KeyedVectors\n",
    "    from pathlib import Path\n",
    "\n",
    "    glove_data_file = 'glove.6B.%dd.txt' % dim\n",
    "    word2vec_output_file = '%s.w2v' % glove_data_file\n",
    "\n",
    "    if not Path(word2vec_output_file).exists():\n",
    "        glove2word2vec(glove_input_file=glove_data_file, word2vec_output_file=word2vec_output_file)\n",
    "    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    return glove_model\n",
    "\n",
    "# We will use v2, because it is more versatile\n",
    "dim = 50\n",
    "model = load_glove_model_v2(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the embedding matrix for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an embedding matrix out of embedding vectors\n",
    "embedding_matrix = np.zeros((len(model.vocab), dim))\n",
    "for i in range(len(model.vocab)):\n",
    "    embedding_vector = model[model.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run this embedding model on a sample sentence within TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/\n",
    "\n",
    "# Print a lookup for a sample sentence with word IDs [1, 5, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sentences = tf.placeholder(tf.int32, shape=[None,None])\n",
    "\n",
    "saved_embeddings = tf.constant(embedding_matrix)\n",
    "embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "embedding_lookup = tf.nn.embedding_lookup(embedding, sentences)\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(embedding_lookup,\n",
    "                       feed_dict={sentences:[[1, 5, 10]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works.  Next, we will convert our dataset into a form suitable for this training.  We should also experiment with [GoogleNews word2vec dataset](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/), because that is more relevant to a Fake News project.\n",
    "\n",
    "### Look up embeddings for our dataset\n",
    "\n",
    "We first load our data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def get_word_list(article):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    words_only = re.sub(strip_special_chars, \"\", article.lower())\n",
    "    return words_only.split()\n",
    "\n",
    "def tabulate_data(dataset_name):\n",
    "    \"\"\"Create a Pandas dataframe out of input Perez-Rosas dataset files\n",
    "    @param dataset_name: Name of the dataset (fakenews or celebrity)\n",
    "    @returns Pandas dataframe with columns:\n",
    "        dataset_name, news_type, news_category, news_headline, news_content\n",
    "    \"\"\"\n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "\n",
    "    result_data_list = []\n",
    "    data_dir = '../../data/fakeNewsDatasets_Perez-Rosas2018'\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data['news_word_list'] = get_word_list(news_content_data)\n",
    "                result_data['num_words'] = len(result_data['news_word_list'])\n",
    "                result_data_list.append(result_data)\n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df = tabulate_data('fakeNewsDataset')\n",
    "fakenews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look up each word in the vocabulary of our embedding model.  If found, we note the index within the vocabulary.\n",
    "\n",
    "To make this fast, we use the vectorized \"apply\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_UNKNOWN = 99\n",
    "def word_to_id(word_list):\n",
    "    word_index_list = []\n",
    "    for word in word_list:\n",
    "        if word in model.vocab:\n",
    "            word_index_list.append(model.vocab[word].index)\n",
    "        else:\n",
    "            # Unknown\n",
    "            word_index_list.append(ID_UNKNOWN)\n",
    "    return word_index_list\n",
    "fakenews_df['embedding_ids'] = fakenews_df['news_word_list'].apply(lambda x: word_to_id(x))\n",
    "fakenews_df['num_embedding_ids'] = fakenews_df['embedding_ids'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to print the embeddings for one of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_article = fakenews_df['embedding_ids'].tolist()[0]\n",
    "print(sample_article)\n",
    "\n",
    "import tensorflow  as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sentences = tf.placeholder(tf.int32, shape=[None,None])\n",
    "\n",
    "saved_embeddings = tf.constant(embedding_matrix)\n",
    "embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)\n",
    "embedding_lookup = tf.nn.embedding_lookup(embedding, sentences)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(sess.run(embedding_lookup,\n",
    "                       feed_dict={sentences: [sample_article]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!  Let's try to train a neural net with the data set.  We will reuse some of the code from W266 Assignment 3.\n",
    "\n",
    "First, couple utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.01):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "        \n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        cost = 0.0\n",
    "                \n",
    "        if train:\n",
    "            cost, final_h, train_op = session.run([lm.train_loss_, lm.final_h_, lm.train_step_], feed_dict)\n",
    "        else:\n",
    "            cost, final_h, train_op = session.run([lm.loss_, lm.final_h_, tf.no_op()], feed_dict)\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.01, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "def article_to_ids(articles):\n",
    "    all_ids = []\n",
    "    for article in articles:\n",
    "        all_ids.extend(article)\n",
    "    return np.array(all_ids)\n",
    "\n",
    "def train_test_split(df, id_list_column, split_frac=0.8, do_shuffle=True):\n",
    "    articles = np.array(list(df[id_list_column]), dtype=object)\n",
    "    if do_shuffle:\n",
    "        rng = np.random.RandomState(do_shuffle)\n",
    "        rng.shuffle(articles)  # in-place\n",
    "    train_split = int(split_frac * articles.size)\n",
    "    train_ids = article_to_ids(articles[:train_split])\n",
    "    test_ids = article_to_ids(articles[train_split:])\n",
    "    return train_ids, test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to train the model.  We will use the embeddings instead of the words themselves.  Let's set up some parameters, and then run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=len(model.vocab), \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/final_project\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "\n",
    "train_ids, test_ids = train_test_split(fakenews_df, 'embedding_ids')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to debug upon exception\n",
    "# %pdb\n",
    "\n",
    "reload(rnnlm)\n",
    "\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        print(type(train_ids))\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, train=True)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run predictions, look at perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
