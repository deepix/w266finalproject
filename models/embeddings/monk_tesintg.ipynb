{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting words and vectors seperate\n",
    "\n",
    "#embedding available here\n",
    "#https://nlp.stanford.edu/projects/glove/  \n",
    "#In this tutorial i am using glove.6B.100d.txt\n",
    "\n",
    "word_list=[]\n",
    "word_embedding=[]\n",
    "with open('glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        data=line.split()\n",
    "        word_list.append(data[0])\n",
    "        word_embedding.append([float(i) for i in data[1:]])\n",
    "\n",
    "\n",
    "np.save('word_list',word_list)\n",
    "np.save('word_embedding',word_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading word_list \n",
    "#loading word_embedding\n",
    "\n",
    "word_list=np.load('word_list.npy')\n",
    "word_embedding=np.load('word_embedding.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WARNING', 'The', 'notebook', 'server', 'is', 'listening', 'on', 'all', 'IP', 'addresses', 'and', 'not', 'using', 'encryption', 'This', 'is', 'not', 'recommended']\n",
      "['This', 'isnt', 'the', 'comedic', 'Robin', 'Williams', 'nor', 'is', 'it', 'the', 'quirky', 'insane', 'Robin', 'Williams', 'of', 'recent', 'thriller', 'fame', 'This', 'is', 'a', 'hybrid', 'of', 'the', 'classic', 'drama', 'without', 'over', 'dramatization', 'mixed', 'with', 'Robins', 'new', 'love', 'of', 'the', 'thriller', 'But', 'this', 'isnt', 'a', 'thriller', 'per', 'se', 'This', 'is', 'more', 'a', 'mystery', 'suspense', 'vehicle', 'through', 'which', 'Williams', 'attempts', 'to', 'locate', 'a', 'sick', 'boy', 'and', 'his', 'keeper', 'Also', 'starring', 'Sandra', 'Oh', 'and', 'Rory', 'Culkin', 'this', 'Suspense', 'Drama', 'plays', 'pretty', 'much', 'like', 'a', 'news', 'report', 'until', 'Williams', 'character', 'gets', 'close', 'to', 'achieving', 'his', 'goal', 'I', 'must', 'say', 'that', 'I', 'was', 'highly', 'entertained', 'though', 'this', 'movie', 'fails', 'to', 'teach', 'guide', 'inspect', 'or', 'amuse', 'It', 'felt', 'more', 'like', 'I', 'was', 'watching', 'a', 'guy', 'Williams', 'as', 'he', 'was', 'actually', 'performing', 'the', 'actions', 'from', 'a', 'third', 'person', 'perspective', 'In', 'other', 'words', 'it', 'felt', 'real', 'and', 'I', 'was', 'able', 'to', 'subscribe', 'to', 'the', 'premise', 'of', 'the', 'story', 'All', 'in', 'all', 'its', 'worth', 'a', 'watch', 'though', 'its', 'definitely', 'not', 'Friday', 'Saturday', 'night', 'fare', 'It', 'rates', 'a', '77', '10', 'from', 'the', 'Fiend']\n",
      "[[1975, 0, 12060, 8069, 14, 6146, 13, 64, 13494, 7802, 5, 36, 622, 20099, 37, 14, 36, 3885], [37, 228955, 0, 21698, 4972, 1317, 2094, 14, 20, 0, 17151, 14916, 4972, 1317, 3, 397, 8965, 3152, 37, 14, 7, 7008, 3, 0, 2392, 2692, 296, 74, 61025, 2195, 17, 29196, 50, 835, 3, 0, 8965, 34, 37, 228955, 7, 8965, 532, 7366, 37, 14, 56, 7, 5351, 17495, 1907, 131, 42, 1317, 2444, 4, 10170, 7, 4478, 1606, 5, 26, 7529, 52, 3935, 10105, 3202, 5, 18534, 52493, 37, 17495, 2692, 1381, 1922, 181, 117, 7, 172, 255, 207, 1317, 1395, 1666, 383, 4, 7090, 26, 715, 41, 390, 203, 12, 41, 15, 1786, 18915, 413, 37, 1005, 6186, 4, 5293, 3372, 12000, 46, 46183, 20, 1349, 56, 117, 41, 15, 2641, 7, 1856, 1317, 19, 18, 15, 1403, 3349, 0, 1970, 25, 7, 245, 899, 5251, 6, 68, 1374, 20, 1349, 567, 5, 41, 15, 667, 4, 20769, 4, 0, 11932, 3, 0, 523, 64, 6, 64, 47, 1089, 7, 1716, 413, 47, 3936, 36, 185, 277, 364, 6769, 20, 864, 7, 5599, 206, 25, 0, 53727]]\n"
     ]
    }
   ],
   "source": [
    "#data clearning\n",
    "\n",
    "#converting text to vectors\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "data = [\n",
    "    \"\"\"WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\"\"\",\n",
    "    \"\"\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\"\"\"]\n",
    "\n",
    "final_ = []\n",
    "\n",
    "\n",
    "\n",
    "def get_final(data_line):\n",
    "    sub_final = []\n",
    "    fg = \" \".join(re.split(r'\\/|-', data_line))\n",
    "    data_line=fg.replace('<br  >', ' ')\n",
    "\n",
    "\n",
    "    d = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "    new = data_line.translate(d).split()\n",
    "    print(new)\n",
    "    for i in new:\n",
    "        try:\n",
    "            sub_final.append(np.array(word_list).tolist().index(i.lower()))\n",
    "        except ValueError:\n",
    "            sub_final.append(399999)\n",
    "    final_.append(sub_final)\n",
    "\n",
    "\n",
    "for ii in data:\n",
    "    get_final(ii)\n",
    "\n",
    "print(final_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.2989   -0.60051   0.48185  ...  0.095815  0.10687   0.025202]\n",
      "  [-0.038194 -0.24487   0.72812  ... -0.1459    0.8278    0.27062 ]\n",
      "  [-0.43801   0.49572   1.1181   ... -0.43953   1.2514    0.070576]\n",
      "  ...\n",
      "  [-0.54264   0.41476   1.0322   ... -1.2969    0.76217   0.46349 ]\n",
      "  [-0.19104   0.17601   0.3692   ... -0.5968    0.080843  0.27866 ]\n",
      "  [-0.38725  -0.38811  -0.49279  ... -0.099383  0.18399  -0.083023]]]\n",
      "[[[-0.57058   0.44183   0.70102  ... -0.66102   0.47197   0.37253 ]\n",
      "  [-0.097035  0.23205  -0.010545 ... -0.084817 -0.45299   0.31053 ]\n",
      "  [-0.038194 -0.24487   0.72812  ... -0.1459    0.8278    0.27062 ]\n",
      "  ...\n",
      "  [ 0.30731   0.24737   0.68231  ...  0.13735   1.0103   -0.77614 ]\n",
      "  [-0.038194 -0.24487   0.72812  ... -0.1459    0.8278    0.27062 ]\n",
      "  [ 0.15544  -0.25449   0.7368   ... -0.25619   0.36918  -0.55068 ]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_x=tf.placeholder(tf.int32,shape=[None,None])\n",
    "\n",
    "Word_embedding = tf.get_variable(name=\"W\", shape=[400000,100], initializer=tf.constant_initializer(np.array(word_embedding)), trainable=False)\n",
    "embedding_loopup= tf.nn.embedding_lookup(Word_embedding,input_x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ii in final_:\n",
    "            print(sess.run(embedding_loopup,feed_dict={input_x:[ii]}))\n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2989    -0.60051    0.48185   -0.13038   -0.068388   0.16441\n",
      " -0.34119   -0.52143   -0.025612   0.087871  -0.034554  -0.26052\n",
      " -0.1239     0.10174    0.54033    0.26336   -0.29377   -0.37216\n",
      " -0.33782   -0.28566    0.0096186 -0.20075   -0.1592     0.027219\n",
      " -0.20424    0.59778   -0.48239    0.90715   -0.36636   -0.1993\n",
      "  0.84447   -0.15136   -0.097391   0.78983   -0.34897    0.028516\n",
      "  0.045391  -0.39529    0.33923    0.04998   -1.1097     0.22696\n",
      "  0.13564   -0.18441    0.78137   -0.22849    0.4151     0.13281\n",
      " -0.19269   -0.96898    0.9596     0.36388   -0.12575    0.81254\n",
      "  0.13076   -1.892      0.33793   -0.15247    1.8207     0.18241\n",
      "  0.027438   0.3785    -0.25259   -0.12538   -0.074345   0.2165\n",
      " -0.831      0.31736    0.37616   -0.27634   -0.42189   -0.28225\n",
      " -0.11396   -0.80373    0.45635    0.82757    0.70023    0.49755\n",
      " -0.64901   -0.24195    1.0454    -1.4573    -0.97332   -0.051946\n",
      " -1.2024    -0.4798     0.33814    0.24562    0.27226   -0.17699\n",
      "  0.053299   0.39419   -0.34059   -0.23434   -0.25774    0.62146\n",
      " -0.31351    0.095815   0.10687    0.025202 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#validating result by manual embedding lookup\n",
    "\n",
    "print(word_embedding[1975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
